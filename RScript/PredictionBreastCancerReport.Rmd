---
title: "STA691 Project"
author: "Mamun, Ruthwik,Ayman"
date: "`r Sys.Date()`"
output: word_document
---



```{r}
library(ISLR2)
library(ggplot2)
library(archive)
library(corrplot)
library(dplyr)

url = "https://archive.ics.uci.edu/static/public/17/breast+cancer+wisconsin+diagnostic.zip"
CancerData <- read.csv(archive_read(url,1), header=FALSE, stringsAsFactors = FALSE)
names(CancerData) = c("ID", "Diagnosis", "radius_mean", "texture_mean", "perimeter_mean", "area_mean", "smoothness_mean", "compactness_mean", "concavity_mean", "concave_points_mean", "symmetry_mean", "fractal_dimension_mean", "radius_sd", "texture_sd", "perimeter_sd", "area_sd", "smoothness_sd", "compactness_sd", "concavity_sd", "concave_points_sd", "symmetry_sd", "fractal_dimension_sd", "radius_worst", "texture_worst", "perimeter_worst", "area_worst", "smoothness_worst", "compactness_worst", "concavity_worst", "concave_points_worst", "symmetry_worst", "fractal_dimension_worst")
```

```{r}
library(corrplot)
data.frame(CancerData)
sum(is.na(CancerData))
summary(CancerData)

correlation_matrix<-cor(CancerData[,-2])
corrplot(correlation_matrix, method = "color", type = "upper", tl.col = "black", tl.srt = 45)

apply(CancerData,2, sd)
table(CancerData$Diagnosis)
names(CancerData)
```
```{r}

numeric_cols <- sapply(CancerData, is.numeric)
numeric_cols <- names(numeric_cols)[!names(numeric_cols) %in% c("ID", "Diagnosis")]

# Set up the plotting layout with appropriate margins
par(mfrow = c(6, 5), mar = c(3, 3, 1, 1))

# Loop through each numeric column and plot a histogram
for (col in numeric_cols) {
  hist(CancerData[[col]], main = col, xlab = "", col = "blue")
}

# Reset the plotting layout
par(mfrow = c(1, 1))
```
```{r}
numeric_data <- CancerData[, sapply(CancerData, is.numeric)]
summary_stats <- rbind(
  count = colSums(!is.na(numeric_data)),
  mean = colMeans(numeric_data, na.rm = TRUE),
  median = apply(numeric_data, 2, median, na.rm = TRUE),
  sd = apply(numeric_data, 2, sd, na.rm = TRUE),
  min = apply(numeric_data, 2, min, na.rm = TRUE),
  max = apply(numeric_data, 2, max, na.rm = TRUE),
  q25 = apply(numeric_data, 2, quantile, probs = 0.25, na.rm = TRUE),
  q75 = apply(numeric_data, 2, quantile, probs = 0.75, na.rm = TRUE),
  IQR = apply(numeric_data, 2, IQR, na.rm = TRUE)
)

# Transpose the summary statistics
summary_stats_transposed <- t(summary_stats)

# Print the transposed summary statistics
print(summary_stats_transposed)
```
```{r}
summary(CancerData["Diagnosis"])
ggplot(CancerData, aes(x = Diagnosis))+geom_bar(fill = "blue", width=0.1)+geom_text(stat = 'count', aes(label = ..count..), vjust = -0.3)+labs(title = "Diagnosis Distribution", x = "Diagnosis", y = "Number of Cases")
```
```{r}

```

```{r}

# Sample code for generating a 30-dimensional plot using PCA and subsets

# Load required libraries
library(ggplot2)
library(dplyr)
library(MASS) # for generating multivariate normal data
library(RColorBrewer) # for color palettes
#CancerData$Diagnosis <- ifelse(CancerData$Diagnosis == "B", "Benign", "Malignant")

# Perform PCA for dimensionality reduction
pca_result <- prcomp(CancerData[, -c(1:2)], scale = TRUE)

# Plot the first two principal components
pca_data <- as.data.frame(pca_result$x[, 1:2])
pca_data$class <- CancerData$Diagnosis

# Color palette for classes
colors <- c("Malignant" = "red", "Benign" = "green")

# Plot
ggplot(pca_data, aes(PC1, PC2, color = class, shape = class)) +
  geom_point(size = 2) +
  scale_color_manual(values = colors) +
  scale_shape_manual(values = c(16, 17)) +  # using different shapes for each class
  labs(title = "First two Principal Components plot with class labels") +
  theme_minimal()
```

###  With a total of 569 cases for the response variable 'Diagnosis'â€”comprising 357 benign and 212 malignant cases. 


### In light of the dataset's features having diverse measurement units, we opted to standardize our data before modeling.
```{r}
#standardizing data
set.seed(12)
CancerData$Diagnosis <- ifelse(CancerData$Diagnosis == "B", "Benign", "Malignant")
CancerData$Diagnosis <- as.factor(CancerData$Diagnosis)
CancerData1<- data.frame(CancerData)[,-1]
scaleX<-data.frame(scale(CancerData1[,-1]))
CancerData_model<-cbind(scaleX,CancerData1[,1])
names(CancerData_model)[names(CancerData_model)=="CancerData1[, 1]"]="Diagnosis"
indices<- sample(1:nrow(CancerData_model), 0.70*nrow(CancerData_model))
TrainData <- CancerData_model[indices,]
TestData <- CancerData_model[-indices,]
table(TrainData$Diagnosis)
table(TestData$Diagnosis)
```

```{r, eval=FALSE}
#Logistic regression 

fit_logistic <- glm(Diagnosis ~ ., data = TrainData, family = binomial)
library(car)
data.frame(vif(fit_logistic))
```

### Logistic regression was not selected as the modeling approach for this project due to the presence of extreme multicollinearity among the features in the dataset. The high degree of multicollinearity hindered the convergence of the logistic regression model during training. Additionally, concerns about potential overfitting further influenced the decision to explore alternative modeling techniques better suited to handling multicollinearity issues.


```{r}
set.seed(12)
#LDA with the original Data
library(MASS)
lda_model <- lda(Diagnosis ~ ., data = TrainData)
predictions_lda <- predict(lda_model, newdata = TestData)
prediction_lda_cm<-table(predictions_lda$class,TestData$Diagnosis)
prediction_lda_er<-(prediction_lda_cm[1,2]+prediction_lda_cm[2,1])/sum(prediction_lda_cm)

#QDA with the original Data
library(MASS)
qda_model <- qda(Diagnosis ~ ., data = TrainData)
predictions_qda <- predict(qda_model, newdata = TestData)
prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)

#Naive Bayes
library(e1071)
nb_model <- naiveBayes(Diagnosis ~ ., data = TrainData)
predictions_nb <- predict(nb_model, newdata = TestData)
prediction_nb_cm<-table(predictions_nb,TestData$Diagnosis)
prediction_nb_er<-(prediction_nb_cm[1,2]+prediction_nb_cm[2,1])/sum(prediction_nb_cm)

#KNN with the original dataset (Best k=9: reference, previous chunk)

library(class)
library(caret)
train.x=data.frame(TrainData[,1:30])
test.x=data.frame(TestData[,1:30])
train.y=factor(TrainData[,31])
test.y=factor(TestData[,31])
KNN.fit=knn(train.x, test.x, train.y)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']


#Decision Tree
library(rpart)
dt_model <- rpart(Diagnosis ~ ., data = TrainData)
predictions_dt <- predict(dt_model, newdata = TestData, type = "class")
prediction_dt_cm<-table(predictions_dt,TestData$Diagnosis)
prediction_dt_er<-(prediction_dt_cm[1,2]+prediction_dt_cm[2,1])/sum(prediction_dt_cm)

#RandomForest with the original Data

library(randomForest)
rf_model <- randomForest(Diagnosis ~ ., data = TrainData)
predictions_rf <- predict(rf_model, newdata = TestData)
prediction_rf_cm<-table(predictions_rf,TestData$Diagnosis)
prediction_rf_er<-(prediction_rf_cm[1,2]+prediction_rf_cm[2,1])/sum(prediction_rf_cm)


#Gradient Boosting Machines (GBM)
TrainData_boosting<-TrainData
TrainData_boosting$Diagnosis <- as.integer(TrainData_boosting$Diagnosis == "Malignant")
      #"M" will be converted to 1 and "B" to 0
library(gbm)
boosting_model <- gbm(Diagnosis ~ ., distribution = "bernoulli", data = TrainData_boosting)
predictions_boosting <- predict(boosting_model, newdata = TestData, type = "response")
predicted_classes <- ifelse(predictions_boosting > 0.5, levels(as.factor(TestData$Diagnosis))[2], levels(as.factor(TestData$Diagnosis))[1])
prediction_boosting_cm<-table(predicted_classes,TestData$Diagnosis)
prediction_boosting_er<-(prediction_boosting_cm[1,2]+prediction_boosting_cm[2,1])/sum(prediction_boosting_cm)

#Support Vector Machine (SVM)
library(e1071)
svm_model <- svm(Diagnosis ~ ., data = TrainData, kernel = "radial")
predictions_svm <- predict(svm_model, newdata = TestData)
prediction_svm_cm<-table(predictions_svm,TestData$Diagnosis)
prediction_svm_er<-(prediction_svm_cm[1,2]+prediction_svm_cm[2,1])/sum(prediction_svm_cm)

#printing results
print(paste("Accuracy of LDA:",round((1-prediction_lda_er)*100,2),"%"))
print(paste("Accuracy of QDA:",round((1-prediction_qda_er)*100,2),"%"))
print(paste("Accuracy of Naive Bayes:",round((1-prediction_nb_er)*100,2),"%"))
print(paste("Accuracy of KNN:",round(accuracy_KNN*100,2),"%"))
print(paste("Accuracy of Decision Tree:",round((1-prediction_dt_er)*100,2),"%"))
print(paste("Accuracy of Random Forest:",round((1-prediction_rf_er)*100,2),"%"))
print(paste("Accuracy of Gradient Boosting Machines (GBM):",round((1-prediction_boosting_er)*100,2),"%"))
print(paste("Accuracy of SVM:",round((1-prediction_svm_er)*100,2),"%"))

```
### Based on the initial evaluation of the prediction models fitted to the original dataset with all features, we identified the following best-performing models based on prediction accuracy:

- Quadratic Discriminant Analysis (QDA) with a prediction accuracy of 97.66%
- Support Vector Machine (SVM) also achieved a prediction accuracy of 97.66%
- K-nearest Neighbors (KNN) with a prediction accuracy of 96.49%
- Random Forest with a prediction accuracy of 96.49%

These models demonstrated the highest prediction accuracies among the eight models evaluated. Moving forward, we will focus on further tuning and improving the performance of these selected models to optimize their predictive capabilities for the Diagnosis cases.



## To enhance the performance of the models, we took the following steps into consideration:

### Data balancing: 
Addressing class imbalance through techniques such as undersampling, oversampling, or synthetic data generation to ensure balanced representation of classes and mitigate bias.

### Fine-tuning of parameters: 
Systematically adjusting model hyperparameters through techniques like grid search or randomized search to optimize model performance and generalization.



### To avoid bias towards the Benign group in our training model, we balanced the dataset. To balance the dataset, we employed undersampling, k-medoids clustering, and SMOTE (Synthetic Minority Over-Sampling Technique). 

- Undersampling involved randomly selecting a subset of observations from the majority class (in this case, the Benign group) to match the number of observations in the minority class (the Malignant group). 

- K-medoid clustering is a partitioning method that assigns each data point to the cluster represented by the nearest medoid, which is the most centrally located point within a cluster. We used K-medoid clustering with K=12. 

- SMOTE generates synthetic samples for the minority class to increase their representation in the dataset and improve model performance. These synthetic samples are created by interpolating between existing minority class samples, effectively introducing new data points that resemble the minority class instances. 

## Feature selection using PCA

```{r}
##Feature selection
library(stats)
model_pca <- prcomp(CancerData[,-c(1,2)])
variance_explained <- model_pca$sdev^2 / sum(model_pca$sdev^2)
cumulative_explained_variance <- cumsum(variance_explained)
num_components <- which.max(cumulative_explained_variance >= 0.999)  # Select components explaining 99% of variability. 99.9% variability explained by first three components. 

rotation_matrix <- model_pca$rotation
top_component_loadings <- abs(rotation_matrix[, 1:num_components])
overall_loadings <- apply(top_component_loadings, 1, sum)
top_feature_indices <- unique(order(overall_loadings, decreasing = TRUE)[1:10])
original_features <- colnames(TrainData)[top_feature_indices]
TrainData_PCA10<-cbind(TrainData[,original_features],TrainData[,31])
names(TrainData_PCA10)[names(TrainData_PCA10)=="TrainData[, 31]"]="Diagnosis"
top_feature_indices <- unique(order(overall_loadings, decreasing = TRUE)[1:15])
original_features <- colnames(TrainData)[top_feature_indices]
TrainData_PCA15<-cbind(TrainData[,original_features],TrainData[,31])
names(TrainData_PCA15)[names(TrainData_PCA15)=="TrainData[, 31]"]="Diagnosis"
top_feature_indices <- unique(order(overall_loadings, decreasing = TRUE)[1:20])
original_features <- colnames(TrainData)[top_feature_indices]
TrainData_PCA20<-cbind(TrainData[,original_features],TrainData[,31])
names(TrainData_PCA20)[names(TrainData_PCA20)=="TrainData[, 31]"]="Diagnosis"
top_feature_indices <- unique(order(overall_loadings, decreasing = TRUE)[1:25])
original_features <- colnames(TrainData)[top_feature_indices]
TrainData_PCA25<-cbind(TrainData[,original_features],TrainData[,31])
names(TrainData_PCA25)[names(TrainData_PCA25)=="TrainData[, 31]"]="Diagnosis"


top_feature_indices <- unique(order(overall_loadings, decreasing = TRUE))
original_features <- colnames(TrainData)[top_feature_indices] #list of selected variables from PCA (other than Diagnosis)
#TrainData[,c(original_features[1:3],"Diagnosis")]
```

```{r}
#balancing data set (under-sampling)
set.seed(12)
malignant<- TrainData[which(TrainData$Diagnosis=="Malignant"),]
benign<- TrainData[which(TrainData$Diagnosis=="Benign"),]
benign_indices_US<-sample(1:nrow(benign), 143)
benign_US<- benign[benign_indices_US,]
TrainData_US<-rbind(malignant,benign_US)
table (TrainData_US$Diagnosis)
```

```{r}
#balancing data set (143-medoids clustering)
set.seed(12)
library(factoextra)
library(cluster)
benign_KC<-data.frame(pam(benign, 143)$medoids)
benign_KC$Diagnosis = ifelse(benign_KC$Diagnosis == "1", "Benign", "Malignant")
benign_KC$Diagnosis <- as.factor(benign_KC$Diagnosis)
TrainData_KC<-rbind(malignant,benign_KC)
table (TrainData_KC$Diagnosis)
```
```{r}
#SMOTE (Synthetic Minority Over-Sampling Technique) 
#install.packages("devtools")
#devtools::install_github("cran/DMwR")
```

###############SMOTE SAMPLING########################################################

## SMOT (Synthetic Minority Over-Sampling Technique) with QDA
```{r}
#SMOT  (Synthetic Minority Over-Sampling Technique) with QDA
library(DMwR)
#library(randomForest)
prediction_qda_error= c()
TestData_OS<-TestData
for (i in seq(from = 350, to = 450, by = 1))
  {
  accuracy_qda<-c()
    for (j in 1:10)
    {
    TrainData_OS<-SMOTE(Diagnosis ~ ., TrainData, perc.over = i, k = 5)
    qda_model <- qda(Diagnosis ~ ., data = TrainData_OS)
    predictions_qda <- predict(qda_model, newdata = TestData)
    prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
    prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)
    accuracy_qda[j] <- 1-prediction_qda_er
    }
  prediction_qda_error[i]<-mean(accuracy_qda)
  }
```

```{r}
b=seq(from = 350, to = 450, by = 1)
a=na.omit(prediction_qda_error)[1:length(na.omit(prediction_qda_error))]
top_three_indices<-order(a, decreasing = TRUE)[1:3]
plot(b, a, ylab = "Random Forest prediction accuracy (mean of 30 sample)", xlab="% of synthetic sample generated for the minority class (seq(from = 178, to = 241, by = 1))", ylim = c((min(a) - 0.02), 1))
points(b[top_three_indices], a[top_three_indices], col = "red", pch = 19)
text(b[top_three_indices][1], a[top_three_indices][1], labels = paste(b[top_three_indices][1],round(a[top_three_indices][1],4),sep = ","), pos = 1, col = "blue",cex=0.75)
text(b[top_three_indices][3], a[top_three_indices][3], labels = paste(b[top_three_indices][3],round(a[top_three_indices][3],4),sep = ","), pos = 3, col = "blue",cex=0.75)
text(b[top_three_indices][2], a[top_three_indices][2], labels = paste(b[top_three_indices][2],round(a[top_three_indices][2],4),sep = ","), pos = 4, col = "blue",cex=0.75)
```
```{r}
TrainData_OS<-SMOTE(Diagnosis ~ ., TrainData, perc.over = 380, k = 5)
    qda_model <- qda(Diagnosis ~ ., data = TrainData_OS)
    predictions_qda <- predict(qda_model, newdata = TestData)
    prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
    prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)
    1-prediction_qda_er
    
```
### Using Best SMOTE Trainer, best sample was created and saved. 



## SMOT (Synthetic Minority Over-Sampling Technique) with Random Forest

```{r}
#SMOT  (Synthetic Minority Over-Sampling Technique) with randomForest
set.seed(12)
library(DMwR)
library(randomForest)
prediction_rf_error= c()
TestData_OS<-TestData
for (i in seq(from = 10, to = 2000, by = 1))
  {
  accuracy_rf<-c()
    for (j in 1:30)
    {
    TrainData_OS<-SMOTE(Diagnosis ~ ., TrainData, perc.over = i, k = 5)
    model_rf <- randomForest(Diagnosis ~ ., data = TrainData_OS, ntree = 100)
    predictions <- predict(model_rf, newdata = TestData_OS)
    prediction_rf<-table(predictions,TestData_OS$Diagnosis)
    prediction_rf_er<-(prediction_rf[1,2]+prediction_rf[2,1])/sum(prediction_rf)
    accuracy_rf[j] <- 1-prediction_rf_er
    }
  prediction_rf_error[i]<-mean(accuracy_rf)
  }
```
```{r}
b=seq(from = 178, to = 241, by = 1)
a=na.omit(prediction_rf_error)[1:length(na.omit(prediction_rf_error))]
top_three_indices<-order(a, decreasing = TRUE)[1:3]
plot(b, a, ylab = "Random Forest prediction accuracy (mean of 30 sample)", xlab="% of synthetic sample generated for the minority class (seq(from = 178, to = 241, by = 1))", ylim = c((min(a) - 0.02), 1))
points(b[top_three_indices], a[top_three_indices], col = "red", pch = 19)
text(b[top_three_indices][1], a[top_three_indices][1], labels = paste(b[top_three_indices][1],round(a[top_three_indices][1],4),sep = ","), pos = 1, col = "blue",cex=0.75)
text(b[top_three_indices][3], a[top_three_indices][3], labels = paste(b[top_three_indices][3],round(a[top_three_indices][3],4),sep = ","), pos = 3, col = "blue",cex=0.75)
text(b[top_three_indices][2], a[top_three_indices][2], labels = paste(b[top_three_indices][2],round(a[top_three_indices][2],4),sep = ","), pos = 4, col = "blue",cex=0.75)
```

# Tuning KNN Model 

## Selection of K

```{r, eval=FALSE}
#Tuning KNN model (selecting best K)
set.seed(12)
library(class)
library(caret)
trControl=trainControl(method="repeatedcv",
                       number=5,
                       repeats=10)
# Create a data frame for tuning grid without the 'k' parameter
tuneGrid <- expand.grid(k = 1:30)

fit <- train(Diagnosis ~ .,
             data = TrainData,
             method = 'knn',          # K-nearest neighbors method
             tuneGrid = tuneGrid,    # Grid of hyperparameters to tune
             trControl = trControl)  # Train control parameters

tuning_results <- fit$results
best_k_row <- tuning_results[which.max(tuning_results$Accuracy), ]
best_k_row
best_k_row$k
plot(tuning_results$k,tuning_results$Accuracy, main="KNN - 'K' Selection (10-repeated 5-fold cross-validation)",xlab="#Neighbours", ylab="Accuracy(repeated Cross-validation)")
lines(tuning_results$k,tuning_results$Accuracy,type="l", col="blue")
points(tuning_results$k[which.max(tuning_results$Accuracy)], tuning_results$Accuracy[which.max(tuning_results$Accuracy)], col = "red", pch = 19)
```
### Best k=5

## KNN with the original dataset (Best k=5: reference, previous chunk)
```{r}
#KNN with the original dataset (Best k=5: reference, previous chunk)

library(class)
library(caret)
Train=c("TrainData","TrainData")
train.x=data.frame(TrainData[,1:30])
test.x=data.frame(TestData[,1:30])
train.y=factor(TrainData[,31])
test.y=factor(TestData[,31])
KNN.fit=knn(train.x, test.x, train.y,k=5)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
accuracy_KNN
```
## SMOT (Synthetic Minority Over-Sampling Technique) with KNN

```{r}
#SMOT  (Synthetic Minority Over-Sampling Technique) with KNN
#set.seed(12)
library(DMwR)
#library(randomForest)
prediction_KNN_error= c()
TestData_OS<-TestData
for (i in seq(from = 10, to = 2000, by = 10))
  {
    accuracy_KNN<-c()
    for (j in 1:30)
    {
  TrainData_OS<-SMOTE(Diagnosis ~ ., TrainData, perc.over = i, k = 5)
  train.x=data.frame(TrainData_OS[,1:30])
  test.x=data.frame(TestData_OS[,1:30])
  train.y=factor(TrainData_OS[,31])
  test.y=factor(TestData_OS[,31])
  KNN.fit=knn(train.x, test.x, train.y,k=5)
  cm_KNN<-confusionMatrix(KNN.fit,test.y)
  accuracy_KNN[j] <- cm_KNN$overall['Accuracy']
    }
prediction_KNN_error[i]<-mean(accuracy_KNN)

}
```

```{r}
b=seq(from = 10, to = 2000, by = 10)
a=na.omit(prediction_KNN_error)[1:length(na.omit(prediction_KNN_error))]
top_three_indices<-order(a, decreasing = TRUE)[1:3]
plot(b, a, ylab = "KNN prediction accuracy (mean of 30 SMOT sample)", xlab="% of synthetic sample generated for the minority class (seq(from = 10, to = 2000, by = 10))", ylim = c((min(a) - 0.02), 1))
points(b[top_three_indices], a[top_three_indices], col = "red", pch = 19)
text(b[top_three_indices][1], a[top_three_indices][1], labels = paste(b[top_three_indices][1],round(a[top_three_indices][1],4),sep = ","), pos = 1, col = "blue",cex=0.75)
text(b[top_three_indices][3], a[top_three_indices][3], labels = paste(b[top_three_indices][3],round(a[top_three_indices][3],4),sep = ","), pos = 3, col = "blue",cex=0.75)
text(b[top_three_indices][2], a[top_three_indices][2], labels = paste(b[top_three_indices][2],round(a[top_three_indices][2],4),sep = ","), pos =4, col = "blue",cex=0.75)
```
## SMOT (Synthetic Minority Over-Sampling Technique) with SVM
```{r}
#SMOT  (Synthetic Minority Over-Sampling Technique) with SVM
library(DMwR)
#library(randomForest)
prediction_svm_error= c()
TestData_OS<-TestData
for (i in seq(from = 180, to = 230, by = 1))
  {
  accuracy_svm<-c()
    for (j in 1:30)
    {
    TrainData_OS<-SMOTE(Diagnosis ~ ., TrainData, perc.over = i, k = 5)
    svm_model <- svm(Diagnosis ~ ., data = TrainData_OS, kernel = "radial")
    predictions_svm <- predict(svm_model, newdata = TestData)
    prediction_svm_cm<-table(predictions_svm,TestData$Diagnosis)
  prediction_svm_er<-(prediction_svm_cm[1,2]+prediction_svm_cm[2,1])/sum(prediction_svm_cm)
    accuracy_svm[j] <- 1-prediction_svm_er
    }
  prediction_svm_error[i]<-mean(accuracy_svm)
  }
```

```{r}
b=seq(from = 180, to = 230, by = 1)
a=na.omit(prediction_svm_error)[1:length(na.omit(prediction_svm_error))]
top_three_indices<-order(a, decreasing = TRUE)[1:3]
plot(b, a, ylab = "Random Forest prediction accuracy (mean of 30 sample)", xlab="% of synthetic sample generated for the minority class (seq(from = 178, to = 241, by = 1))", ylim = c((min(a) - 0.02), 1))
points(b[top_three_indices], a[top_three_indices], col = "red", pch = 19)
text(b[top_three_indices][1], a[top_three_indices][1], labels = paste(b[top_three_indices][1],round(a[top_three_indices][1],4),sep = ","), pos = 3, col = "blue",cex=0.75)
text(b[top_three_indices][3], a[top_three_indices][3], labels = paste(b[top_three_indices][3],round(a[top_three_indices][3],4),sep = ","), pos = 3, col = "blue",cex=0.75)
text(b[top_three_indices][2], a[top_three_indices][2], labels = paste(b[top_three_indices][2],round(a[top_three_indices][2],4),sep = ","), pos = 4, col = "blue",cex=0.75)
```
###################################################################################


## QDA Final Tuning

```{r}
##QDA with selected Features
library(MASS)
qda_model <- qda(Diagnosis ~ ., data = TrainData)
predictions_qda <- predict(qda_model, newdata = TestData)
prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)
print(paste("Accuracy of QDA with all features (original data)):",round((1-prediction_qda_er)*100,2),"%"))

qda_model <- qda(Diagnosis ~ ., data = TrainData[,c(original_features[1:10], "Diagnosis")])
predictions_qda <- predict(qda_model, newdata = TestData)
prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)
print(paste("Accuracy of QDA with top 10 PCA feature (original data):",round((1-prediction_qda_er)*100,2),"%"))
qda_model <- qda(Diagnosis ~ ., data = TrainData[,c(original_features[1:15], "Diagnosis")])
predictions_qda <- predict(qda_model, newdata = TestData)
prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)
print(paste("Accuracy of QDA with top 15 PCA feature (original data):",round((1-prediction_qda_er)*100,2),"%"))
qda_model <- qda(Diagnosis ~ ., data = TrainData[,c(original_features[1:20], "Diagnosis")])
predictions_qda <- predict(qda_model, newdata = TestData)
prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)
print(paste("Accuracy of QDA with top 20 PCA feature (original data):",round((1-prediction_qda_er)*100,2),"%"))
qda_model <- qda(Diagnosis ~ ., data = TrainData[,c(original_features[1:25], "Diagnosis")])
predictions_qda <- predict(qda_model, newdata = TestData)
prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)
print(paste("Accuracy of QDA with top 25 PCA feature (original data):",round((1-prediction_qda_er)*100,2),"%"))

qda_model <- qda(Diagnosis ~ ., data = TrainData_US[,c(original_features[1:30], "Diagnosis")])
predictions_qda <- predict(qda_model, newdata = TestData)
prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)
print(paste("Accuracy of QDA with all features (undersampling data):",round((1-prediction_qda_er)*100,2),"%"))

qda_model <- qda(Diagnosis ~ ., data = TrainData_US[,c(original_features[1:10], "Diagnosis")])
predictions_qda <- predict(qda_model, newdata = TestData)
prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)
print(paste("Accuracy of QDA with top 10 PCA features (undersampling data):",round((1-prediction_qda_er)*100,2),"%"))
qda_model <- qda(Diagnosis ~ ., data = TrainData_US[,c(original_features[1:15], "Diagnosis")])
predictions_qda <- predict(qda_model, newdata = TestData)
prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)
print(paste("Accuracy of QDA with top 15 PCA features (undersampling data):",round((1-prediction_qda_er)*100,2),"%"))
qda_model <- qda(Diagnosis ~ ., data = TrainData_US[,c(original_features[1:20], "Diagnosis")])
predictions_qda <- predict(qda_model, newdata = TestData)
prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)
print(paste("Accuracy of QDA with top 20 PCA features (undersampling data):",round((1-prediction_qda_er)*100,2),"%"))
qda_model <- qda(Diagnosis ~ ., data = TrainData_US[,c(original_features[1:25], "Diagnosis")])
predictions_qda <- predict(qda_model, newdata = TestData)
prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)
print(paste("Accuracy of QDA with top 25 PCA features (undersampling data):",round((1-prediction_qda_er)*100,2),"%"))



qda_model <- qda(Diagnosis ~ ., data = TrainData_KC[,c(original_features[1:30], "Diagnosis")])
predictions_qda <- predict(qda_model, newdata = TestData)
prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)
print(paste("Accuracy of QDA with all features (k-medoid clustering):",round((1-prediction_qda_er)*100,2),"%"))

qda_model <- qda(Diagnosis ~ ., data = TrainData_KC[,c(original_features[1:10], "Diagnosis")])
predictions_qda <- predict(qda_model, newdata = TestData)
prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)
print(paste("Accuracy of QDA with top 10 PCA features (k-medoid clustering):",round((1-prediction_qda_er)*100,2),"%"))
qda_model <- qda(Diagnosis ~ ., data = TrainData_KC[,c(original_features[1:15], "Diagnosis")])
predictions_qda <- predict(qda_model, newdata = TestData)
prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)
print(paste("Accuracy of QDA with top 15 PCA features (k-medoid clustering):",round((1-prediction_qda_er)*100,2),"%"))
qda_model <- qda(Diagnosis ~ ., data = TrainData_KC[,c(original_features[1:20], "Diagnosis")])
predictions_qda <- predict(qda_model, newdata = TestData)
prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)
print(paste("Accuracy of QDA with top 20 PCA features (k-medoid clustering):",round((1-prediction_qda_er)*100,2),"%"))
qda_model <- qda(Diagnosis ~ ., data = TrainData_KC[,c(original_features[1:25], "Diagnosis")])
predictions_qda <- predict(qda_model, newdata = TestData)
prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)
print(paste("Accuracy of QDA with top 25 PCA features (k-medoid clustering):",round((1-prediction_qda_er)*100,2),"%"))


TrainData_SMOTE <- data.frame(read.csv("TrainData_SMOTE_qda.csv"))
TrainData_SMOTE$Diagnosis <- as.factor(TrainData_SMOTE$Diagnosis)

qda_model <- qda(Diagnosis ~ ., data = TrainData_SMOTE)
predictions_qda <- predict(qda_model, newdata = TestData)
prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)
print(paste("Accuracy of QDA with all features (SMOT sample)):",round((1-prediction_qda_er)*100,2),"%"))

qda_model <- qda(Diagnosis ~ ., data = TrainData_SMOTE[,c(original_features[1:10], "Diagnosis")])
predictions_qda <- predict(qda_model, newdata = TestData)
prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)
print(paste("Accuracy of QDA with to 10 PCA features (SMOT sample)):",round((1-prediction_qda_er)*100,2),"%"))
qda_model <- qda(Diagnosis ~ ., data = TrainData_SMOTE[,c(original_features[1:15], "Diagnosis")])
predictions_qda <- predict(qda_model, newdata = TestData)
prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)
print(paste("Accuracy of QDA with to 15 PCA features (SMOT sample)):",round((1-prediction_qda_er)*100,2),"%"))
qda_model <- qda(Diagnosis ~ ., data = TrainData_SMOTE[,c(original_features[1:20], "Diagnosis")])
predictions_qda <- predict(qda_model, newdata = TestData)
prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)
print(paste("Accuracy of QDA with to 20 PCA features (SMOT sample)):",round((1-prediction_qda_er)*100,2),"%"))
qda_model <- qda(Diagnosis ~ ., data = TrainData_SMOTE[,c(original_features[1:25], "Diagnosis")])
predictions_qda <- predict(qda_model, newdata = TestData)
prediction_qda_cm<-table(predictions_qda$class,TestData$Diagnosis)
prediction_qda_er<-(prediction_qda_cm[1,2]+prediction_qda_cm[2,1])/sum(prediction_qda_cm)
print(paste("Accuracy of QDA with to 25 PCA features (SMOT sample)):",round((1-prediction_qda_er)*100,2),"%"))

table(TrainData_SMOTE$Diagnosis)

```

## KNN final Tuning

```{r}

set.seed(12)
train.x=data.frame(TrainData[,1:30])
test.x=data.frame(TestData[,1:30])
train.y=factor(TrainData[,31])
test.y=factor(TestData[,31])
KNN.fit=knn(train.x, test.x, train.y)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with all features (original data):",round((accuracy_KNN)*100,2),"%"))


KNN.fit=knn(train.x, test.x, train.y,k=5)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with all features and besk k (original data):",round((accuracy_KNN)*100,2),"%"))


KNN.fit=knn(train.x[,original_features[1:10]], test.x[,original_features[1:10]], train.y,k=5)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with top 10 PCA features and besk k (original data):",round((accuracy_KNN)*100,2),"%"))


KNN.fit=knn(train.x[,original_features[1:15]], test.x[,original_features[1:15]], train.y,k=5)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with top 15 PCA features and besk k (original data):",round((accuracy_KNN)*100,2),"%"))


KNN.fit=knn(train.x[,original_features[1:20]], test.x[,original_features[1:20]], train.y,k=5)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with top 20 PCA features and besk k (original data):",round((accuracy_KNN)*100,2),"%"))


KNN.fit=knn(train.x[,original_features[1:25]], test.x[,original_features[1:25]], train.y,k=5)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with top 25 PCA features and besk k (original data):",round((accuracy_KNN)*100,2),"%"))

train.x=data.frame(TrainData_US[,1:30])
test.x=data.frame(TestData[,1:30])
train.y=factor(TrainData_US[,31])
test.y=factor(TestData[,31])

KNN.fit=knn(train.x[,original_features], test.x[,original_features], train.y,k=5)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with all features and besk k (under sampling):",round((accuracy_KNN)*100,2),"%"))

KNN.fit=knn(train.x[,original_features[1:10]], test.x[,original_features[1:10]], train.y,k=5)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with top 10 PCA features and besk k (under sampling):",round((accuracy_KNN)*100,2),"%"))

KNN.fit=knn(train.x[,original_features[1:15]], test.x[,original_features[1:15]], train.y,k=5)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with top 15 PCA features and besk k (under sampling):",round((accuracy_KNN)*100,2),"%"))

KNN.fit=knn(train.x[,original_features[1:20]], test.x[,original_features[1:20]], train.y,k=5)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with top 20 PCA features and besk k (under sampling):",round((accuracy_KNN)*100,2),"%"))

KNN.fit=knn(train.x[,original_features[1:25]], test.x[,original_features[1:25]], train.y,k=5)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with top 25 PCA features and besk k (under sampling):",round((accuracy_KNN)*100,2),"%"))

train.x=data.frame(TrainData_KC[,1:30])
test.x=data.frame(TestData[,1:30])
train.y=factor(TrainData_KC[,31])
test.y=factor(TestData[,31])

KNN.fit=knn(train.x[,original_features], test.x[,original_features], train.y,k=5)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with all the features and besk k (k-medoid clustering):",round((accuracy_KNN)*100,2),"%"))

KNN.fit=knn(train.x[,original_features[1:10]], test.x[,original_features[1:10]], train.y,k=5)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with top 10 PCA features and besk k (k-medoid clustering):",round((accuracy_KNN)*100,2),"%"))

KNN.fit=knn(train.x[,original_features[1:15]], test.x[,original_features[1:15]], train.y,k=5)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with top 15 PCA features and besk k (k-medoid clustering):",round((accuracy_KNN)*100,2),"%"))

KNN.fit=knn(train.x[,original_features[1:20]], test.x[,original_features[1:20]], train.y,k=5)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with top 20 PCA features and besk k (k-medoid clustering):",round((accuracy_KNN)*100,2),"%"))

KNN.fit=knn(train.x[,original_features[1:25]], test.x[,original_features[1:25]], train.y,k=5)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with top 25 PCA features and besk k (k-medoid clustering):",round((accuracy_KNN)*100,2),"%"))

TrainData_SMOTE <- data.frame(read.csv("TrainData_SMOTE_KNN.csv"))
TrainData_SMOTE$Diagnosis <- as.factor(TrainData_SMOTE$Diagnosis)
train.x=data.frame(TrainData_SMOTE[,1:30])
TestData_OS<-TestData
test.x=data.frame(TestData_OS[,1:30])
train.y=factor(TrainData_SMOTE[,31])
test.y=factor(TestData_OS[,31])
KNN.fit=knn(train.x, test.x, train.y,k=5)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with SMOTE best sample, all features and best k:",round((accuracy_KNN)*100,2),"%"))

KNN.fit=knn(train.x[,original_features], test.x[,original_features], train.y,k=5)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with all the features and besk k (SMOTE best sample):",round((accuracy_KNN)*100,2),"%"))

KNN.fit=knn(train.x[,original_features[1:10]], test.x[,original_features[1:10]], train.y,k=5)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with top 10 PCA features and besk k (SMOTE best sample):",round((accuracy_KNN)*100,2),"%"))

KNN.fit=knn(train.x[,original_features[1:15]], test.x[,original_features[1:15]], train.y,k=5)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with top 15 PCA features and besk k (SMOTE best sample):",round((accuracy_KNN)*100,2),"%"))

KNN.fit=knn(train.x[,original_features[1:20]], test.x[,original_features[1:20]], train.y,k=5)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with top 20 PCA features and besk k (SMOTE best sample):",round((accuracy_KNN)*100,2),"%"))

KNN.fit=knn(train.x[,original_features[1:25]], test.x[,original_features[1:25]], train.y,k=5)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with top 25 PCA features and besk k (SMOTE best sample):",round((accuracy_KNN)*100,2),"%"))

table(TrainData_SMOTE$Diagnosis)

```

## Random Forest final Tuning

```{r}
set.seed(12)

model_rf <- randomForest(Diagnosis ~ ., data = TrainData, ntree = 100)
predictions <- predict(model_rf, newdata = TestData_OS)
prediction_rf<-table(predictions,TestData_OS$Diagnosis)
prediction_rf_er<-(prediction_rf[1,2]+prediction_rf[2,1])/sum(prediction_rf)
accuracy_rf <- 1-prediction_rf_er
print(paste("Accuracy of Random Forest with all features (original data):",round((accuracy_rf)*100,2),"%"))

set.seed(12)
model_rf <- randomForest(Diagnosis ~ ., data = TrainData[,c(original_features[1:10], "Diagnosis")], ntree = 100)
predictions <- predict(model_rf, newdata = TestData_OS)
prediction_rf<-table(predictions,TestData_OS$Diagnosis)
prediction_rf_er<-(prediction_rf[1,2]+prediction_rf[2,1])/sum(prediction_rf)
accuracy_rf <- 1-prediction_rf_er
print(paste("Accuracy of Random Forest with top 10 PCA features (original data):",round((accuracy_rf)*100,2),"%"))

set.seed(12)
model_rf <- randomForest(Diagnosis ~ ., data = TrainData[,c(original_features[1:15], "Diagnosis")], ntree = 100)
predictions <- predict(model_rf, newdata = TestData_OS)
prediction_rf<-table(predictions,TestData_OS$Diagnosis)
prediction_rf_er<-(prediction_rf[1,2]+prediction_rf[2,1])/sum(prediction_rf)
accuracy_rf <- 1-prediction_rf_er
print(paste("Accuracy of Random Forest with top 15 PCA features (original data):",round((accuracy_rf)*100,2),"%"))

set.seed(12)
model_rf <- randomForest(Diagnosis ~ ., data = TrainData[,c(original_features[1:20], "Diagnosis")], ntree = 100)
predictions <- predict(model_rf, newdata = TestData_OS)
prediction_rf<-table(predictions,TestData_OS$Diagnosis)
prediction_rf_er<-(prediction_rf[1,2]+prediction_rf[2,1])/sum(prediction_rf)
accuracy_rf <- 1-prediction_rf_er
print(paste("Accuracy of Random Forest with top 20 PCA features (original data):",round((accuracy_rf)*100,2),"%"))

set.seed(12)
model_rf <- randomForest(Diagnosis ~ ., data = TrainData[,c(original_features[1:25], "Diagnosis")], ntree = 100)
predictions <- predict(model_rf, newdata = TestData_OS)
prediction_rf<-table(predictions,TestData_OS$Diagnosis)
prediction_rf_er<-(prediction_rf[1,2]+prediction_rf[2,1])/sum(prediction_rf)
accuracy_rf <- 1-prediction_rf_er
print(paste("Accuracy of Random Forest with top 25 PCA features (original data):",round((accuracy_rf)*100,2),"%"))

set.seed(12)
model_rf <- randomForest(Diagnosis ~ ., data = TrainData_US, ntree = 100)
predictions <- predict(model_rf, newdata = TestData_OS)
prediction_rf<-table(predictions,TestData_OS$Diagnosis)
prediction_rf_er<-(prediction_rf[1,2]+prediction_rf[2,1])/sum(prediction_rf)
accuracy_rf <- 1-prediction_rf_er
print(paste("Accuracy of Random Forest with all features (under sampling):",round((accuracy_rf)*100,2),"%"))

set.seed(12)
model_rf <- randomForest(Diagnosis ~ ., data = TrainData_US[,c(original_features[1:10], "Diagnosis")], ntree = 100)
predictions <- predict(model_rf, newdata = TestData_OS)
prediction_rf<-table(predictions,TestData_OS$Diagnosis)
prediction_rf_er<-(prediction_rf[1,2]+prediction_rf[2,1])/sum(prediction_rf)
accuracy_rf <- 1-prediction_rf_er
print(paste("Accuracy of Random Forest with top 10 PCA features (under sampling):",round((accuracy_rf)*100,2),"%"))

set.seed(12)
model_rf <- randomForest(Diagnosis ~ ., data = TrainData_US[,c(original_features[1:15], "Diagnosis")], ntree = 100)
predictions <- predict(model_rf, newdata = TestData_OS)
prediction_rf<-table(predictions,TestData_OS$Diagnosis)
prediction_rf_er<-(prediction_rf[1,2]+prediction_rf[2,1])/sum(prediction_rf)
accuracy_rf <- 1-prediction_rf_er
print(paste("Accuracy of Random Forest with top 15 PCA features (under sampling):",round((accuracy_rf)*100,2),"%"))

set.seed(12)
model_rf <- randomForest(Diagnosis ~ ., data = TrainData_US[,c(original_features[1:20], "Diagnosis")], ntree = 100)
predictions <- predict(model_rf, newdata = TestData_OS)
prediction_rf<-table(predictions,TestData_OS$Diagnosis)
prediction_rf_er<-(prediction_rf[1,2]+prediction_rf[2,1])/sum(prediction_rf)
accuracy_rf <- 1-prediction_rf_er
print(paste("Accuracy of Random Forest with top 20 PCA features (under sampling):",round((accuracy_rf)*100,2),"%"))

set.seed(12)
model_rf <- randomForest(Diagnosis ~ ., data = TrainData_US[,c(original_features[1:25], "Diagnosis")], ntree = 100)
predictions <- predict(model_rf, newdata = TestData_OS)
prediction_rf<-table(predictions,TestData_OS$Diagnosis)
prediction_rf_er<-(prediction_rf[1,2]+prediction_rf[2,1])/sum(prediction_rf)
accuracy_rf <- 1-prediction_rf_er
print(paste("Accuracy of Random Forest with top 25 PCA features (under sampling):",round((accuracy_rf)*100,2),"%"))

set.seed(12)
model_rf <- randomForest(Diagnosis ~ ., data = TrainData_KC, ntree = 100)
predictions <- predict(model_rf, newdata = TestData_OS)
prediction_rf<-table(predictions,TestData_OS$Diagnosis)
prediction_rf_er<-(prediction_rf[1,2]+prediction_rf[2,1])/sum(prediction_rf)
accuracy_rf <- 1-prediction_rf_er
print(paste("Accuracy of Random Forest with all features (K-medoid clustering):",round((accuracy_rf)*100,2),"%"))

set.seed(12)
model_rf <- randomForest(Diagnosis ~ ., data = TrainData_KC[,c(original_features[1:10], "Diagnosis")], ntree = 100)
predictions <- predict(model_rf, newdata = TestData_OS)
prediction_rf<-table(predictions,TestData_OS$Diagnosis)
prediction_rf_er<-(prediction_rf[1,2]+prediction_rf[2,1])/sum(prediction_rf)
accuracy_rf <- 1-prediction_rf_er
print(paste("Accuracy of Random Forest with top 10 PCA features (K-medoid clustering):",round((accuracy_rf)*100,2),"%"))

set.seed(12)
model_rf <- randomForest(Diagnosis ~ ., data = TrainData_KC[,c(original_features[1:15], "Diagnosis")], ntree = 100)
predictions <- predict(model_rf, newdata = TestData_OS)
prediction_rf<-table(predictions,TestData_OS$Diagnosis)
prediction_rf_er<-(prediction_rf[1,2]+prediction_rf[2,1])/sum(prediction_rf)
accuracy_rf <- 1-prediction_rf_er
print(paste("Accuracy of Random Forest with top 15 PCA features (K-medoid clustering):",round((accuracy_rf)*100,2),"%"))

set.seed(12)
model_rf <- randomForest(Diagnosis ~ ., data = TrainData_KC[,c(original_features[1:20], "Diagnosis")], ntree = 100)
predictions <- predict(model_rf, newdata = TestData_OS)
prediction_rf<-table(predictions,TestData_OS$Diagnosis)
prediction_rf_er<-(prediction_rf[1,2]+prediction_rf[2,1])/sum(prediction_rf)
accuracy_rf <- 1-prediction_rf_er
print(paste("Accuracy of Random Forest with top 0 PCA features (K-medoid clustering):",round((accuracy_rf)*100,2),"%"))

set.seed(12)
TrainData_SMOTE <- data.frame(read.csv("TrainData_SMOTE_rf.csv"))
TrainData_SMOTE$Diagnosis <- as.factor(TrainData_SMOTE$Diagnosis)
model_rf <- randomForest(Diagnosis ~ ., data = TrainData_SMOTE, ntree = 100)
predictions <- predict(model_rf, newdata = TestData_OS)
prediction_rf<-table(predictions,TestData_OS$Diagnosis)
prediction_rf_er<-(prediction_rf[1,2]+prediction_rf[2,1])/sum(prediction_rf)
accuracy_rf <- 1-prediction_rf_er
print(paste("Accuracy of Random Forest with SMOTE best sample and all features:",round((accuracy_rf)*100,2),"%"))

set.seed(12)
model_rf <- randomForest(Diagnosis ~ ., data = TrainData_SMOTE[,c(original_features[1:10], "Diagnosis")], ntree = 100)
predictions <- predict(model_rf, newdata = TestData_OS)
prediction_rf<-table(predictions,TestData_OS$Diagnosis)
prediction_rf_er<-(prediction_rf[1,2]+prediction_rf[2,1])/sum(prediction_rf)
accuracy_rf <- 1-prediction_rf_er
print(paste("Accuracy of Random Forest with top 10 PCA features (SMOTE best sample):",round((accuracy_rf)*100,2),"%"))

set.seed(12)
model_rf <- randomForest(Diagnosis ~ ., data = TrainData_SMOTE[,c(original_features[1:15], "Diagnosis")], ntree = 100)
predictions <- predict(model_rf, newdata = TestData_OS)
prediction_rf<-table(predictions,TestData_OS$Diagnosis)
prediction_rf_er<-(prediction_rf[1,2]+prediction_rf[2,1])/sum(prediction_rf)
accuracy_rf <- 1-prediction_rf_er
print(paste("Accuracy of Random Forest with top 15 PCA features (SMOTE best sample):",round((accuracy_rf)*100,2),"%"))

set.seed(12)
model_rf <- randomForest(Diagnosis ~ ., data = TrainData_SMOTE[,c(original_features[1:20], "Diagnosis")], ntree = 100)
predictions <- predict(model_rf, newdata = TestData_OS)
prediction_rf<-table(predictions,TestData_OS$Diagnosis)
prediction_rf_er<-(prediction_rf[1,2]+prediction_rf[2,1])/sum(prediction_rf)
accuracy_rf <- 1-prediction_rf_er
print(paste("Accuracy of Random Forest with top 20 PCA features (SMOTE best sample):",round((accuracy_rf)*100,2),"%"))

set.seed(12)
model_rf <- randomForest(Diagnosis ~ ., data = TrainData_SMOTE[,c(original_features[1:25], "Diagnosis")], ntree = 100)
predictions <- predict(model_rf, newdata = TestData_OS)
prediction_rf<-table(predictions,TestData_OS$Diagnosis)
prediction_rf_er<-(prediction_rf[1,2]+prediction_rf[2,1])/sum(prediction_rf)
accuracy_rf <- 1-prediction_rf_er
print(paste("Accuracy of Random Forest with top 25 PCA features (SMOTE best sample):",round((accuracy_rf)*100,2),"%"))

```

## Fine tuning of SVM


```{r}
set.seed(12)

library(e1071)
svm_model <- svm(Diagnosis ~ ., data = TrainData, kernel = "radial")
predictions_svm <- predict(svm_model, newdata = TestData)
prediction_svm_cm<-table(predictions_svm,TestData$Diagnosis)
prediction_svm_er<-(prediction_svm_cm[1,2]+prediction_svm_cm[2,1])/sum(prediction_svm_cm)
print(paste("Accuracy of SVM with all features (original data):",round((1-prediction_svm_er)*100,2),"%"))

svm_model <- svm(Diagnosis ~ ., data = TrainData[,c(original_features[1:10], "Diagnosis")], kernel = "radial")
predictions_svm <- predict(svm_model, newdata = TestData)
prediction_svm_cm<-table(predictions_svm,TestData$Diagnosis)
prediction_svm_er<-(prediction_svm_cm[1,2]+prediction_svm_cm[2,1])/sum(prediction_svm_cm)
print(paste("Accuracy of SVM with top 10 PCA features (original data):",round((1-prediction_svm_er)*100,2),"%"))

svm_model <- svm(Diagnosis ~ ., data = TrainData[,c(original_features[1:15], "Diagnosis")], kernel = "radial")
predictions_svm <- predict(svm_model, newdata = TestData)
prediction_svm_cm<-table(predictions_svm,TestData$Diagnosis)
prediction_svm_er<-(prediction_svm_cm[1,2]+prediction_svm_cm[2,1])/sum(prediction_svm_cm)
print(paste("Accuracy of SVM with top 15 PCA features (original data):",round((1-prediction_svm_er)*100,2),"%"))

svm_model <- svm(Diagnosis ~ ., data = TrainData[,c(original_features[1:20], "Diagnosis")], kernel = "radial")
predictions_svm <- predict(svm_model, newdata = TestData)
prediction_svm_cm<-table(predictions_svm,TestData$Diagnosis)
prediction_svm_er<-(prediction_svm_cm[1,2]+prediction_svm_cm[2,1])/sum(prediction_svm_cm)
print(paste("Accuracy of SVM with top 20 PCA features (original data):",round((1-prediction_svm_er)*100,2),"%"))

svm_model <- svm(Diagnosis ~ ., data = TrainData[,c(original_features[1:25], "Diagnosis")], kernel = "radial")
predictions_svm <- predict(svm_model, newdata = TestData)
prediction_svm_cm<-table(predictions_svm,TestData$Diagnosis)
prediction_svm_er<-(prediction_svm_cm[1,2]+prediction_svm_cm[2,1])/sum(prediction_svm_cm)
print(paste("Accuracy of SVM with top 25 PCA features (original data):",round((1-prediction_svm_er)*100,2),"%"))

svm_model <- svm(Diagnosis ~ ., data = TrainData_US, kernel = "radial")
predictions_svm <- predict(svm_model, newdata = TestData)
prediction_svm_cm<-table(predictions_svm,TestData$Diagnosis)
prediction_svm_er<-(prediction_svm_cm[1,2]+prediction_svm_cm[2,1])/sum(prediction_svm_cm)
print(paste("Accuracy of SVM with all features (undersampling data):",round((1-prediction_svm_er)*100,2),"%"))

svm_model <- svm(Diagnosis ~ ., data = TrainData_US[,c(original_features[1:10], "Diagnosis")], kernel = "radial")
predictions_svm <- predict(svm_model, newdata = TestData)
prediction_svm_cm<-table(predictions_svm,TestData$Diagnosis)
prediction_svm_er<-(prediction_svm_cm[1,2]+prediction_svm_cm[2,1])/sum(prediction_svm_cm)
print(paste("Accuracy of SVM with top 10 PCA features (undersampling data):",round((1-prediction_svm_er)*100,2),"%"))

svm_model <- svm(Diagnosis ~ ., data = TrainData_US[,c(original_features[1:15], "Diagnosis")], kernel = "radial")
predictions_svm <- predict(svm_model, newdata = TestData)
prediction_svm_cm<-table(predictions_svm,TestData$Diagnosis)
prediction_svm_er<-(prediction_svm_cm[1,2]+prediction_svm_cm[2,1])/sum(prediction_svm_cm)
print(paste("Accuracy of SVM with top 15 PCA features (undersampling data):",round((1-prediction_svm_er)*100,2),"%"))

svm_model <- svm(Diagnosis ~ ., data = TrainData_US[,c(original_features[1:20], "Diagnosis")], kernel = "radial")
predictions_svm <- predict(svm_model, newdata = TestData)
prediction_svm_cm<-table(predictions_svm,TestData$Diagnosis)
prediction_svm_er<-(prediction_svm_cm[1,2]+prediction_svm_cm[2,1])/sum(prediction_svm_cm)
print(paste("Accuracy of SVM with top 20 PCA features (undersampling data):",round((1-prediction_svm_er)*100,2),"%"))

svm_model <- svm(Diagnosis ~ ., data = TrainData_US[,c(original_features[1:25], "Diagnosis")], kernel = "radial")
predictions_svm <- predict(svm_model, newdata = TestData)
prediction_svm_cm<-table(predictions_svm,TestData$Diagnosis)
prediction_svm_er<-(prediction_svm_cm[1,2]+prediction_svm_cm[2,1])/sum(prediction_svm_cm)
print(paste("Accuracy of SVM with top 25 PCA features (undersampling data):",round((1-prediction_svm_er)*100,2),"%"))

svm_model <- svm(Diagnosis ~ ., data = TrainData_KC, kernel = "radial")
predictions_svm <- predict(svm_model, newdata = TestData)
prediction_svm_cm<-table(predictions_svm,TestData$Diagnosis)
prediction_svm_er<-(prediction_svm_cm[1,2]+prediction_svm_cm[2,1])/sum(prediction_svm_cm)
print(paste("Accuracy of SVM with all features (K-medoid clustering):",round((1-prediction_svm_er)*100,2),"%"))

svm_model <- svm(Diagnosis ~ ., data = TrainData_KC[,c(original_features[1:10], "Diagnosis")], kernel = "radial")
predictions_svm <- predict(svm_model, newdata = TestData)
prediction_svm_cm<-table(predictions_svm,TestData$Diagnosis)
prediction_svm_er<-(prediction_svm_cm[1,2]+prediction_svm_cm[2,1])/sum(prediction_svm_cm)
print(paste("Accuracy of SVM with top 10 PCA features ((K-medoid clustering):",round((1-prediction_svm_er)*100,2),"%"))

svm_model <- svm(Diagnosis ~ ., data = TrainData_KC[,c(original_features[1:15], "Diagnosis")], kernel = "radial")
predictions_svm <- predict(svm_model, newdata = TestData)
prediction_svm_cm<-table(predictions_svm,TestData$Diagnosis)
prediction_svm_er<-(prediction_svm_cm[1,2]+prediction_svm_cm[2,1])/sum(prediction_svm_cm)
print(paste("Accuracy of SVM with top 15 PCA features ((K-medoid clustering):",round((1-prediction_svm_er)*100,2),"%"))

svm_model <- svm(Diagnosis ~ ., data = TrainData_KC[,c(original_features[1:20], "Diagnosis")], kernel = "radial")
predictions_svm <- predict(svm_model, newdata = TestData)
prediction_svm_cm<-table(predictions_svm,TestData$Diagnosis)
prediction_svm_er<-(prediction_svm_cm[1,2]+prediction_svm_cm[2,1])/sum(prediction_svm_cm)
print(paste("Accuracy of SVM with top 20 PCA features ((K-medoid clustering):",round((1-prediction_svm_er)*100,2),"%"))

svm_model <- svm(Diagnosis ~ ., data = TrainData_KC[,c(original_features[1:25], "Diagnosis")], kernel = "radial")
predictions_svm <- predict(svm_model, newdata = TestData)
prediction_svm_cm<-table(predictions_svm,TestData$Diagnosis)
prediction_svm_er<-(prediction_svm_cm[1,2]+prediction_svm_cm[2,1])/sum(prediction_svm_cm)
print(paste("Accuracy of SVM with top 25 PCA features ((K-medoid clustering):",round((1-prediction_svm_er)*100,2),"%"))

TrainData_SMOTE <- data.frame(read.csv("TrainData_SMOTE_svm.csv"))
TrainData_SMOTE$Diagnosis <- as.factor(TrainData_SMOTE$Diagnosis)
svm_model <- svm(Diagnosis ~ ., data = TrainData_SMOTE, kernel = "radial")
predictions_svm <- predict(svm_model, newdata = TestData)
prediction_svm_cm<-table(predictions_svm,TestData$Diagnosis)
prediction_svm_er<-(prediction_svm_cm[1,2]+prediction_svm_cm[2,1])/sum(prediction_svm_cm)
accuracy_svm <- 1-prediction_svm_er
print(paste("Accuracy of Random Forest with all features (SMOTE best sample):",round((accuracy_svm)*100,2),"%"))

svm_model <- svm(Diagnosis ~ ., data = TrainData_SMOTE[,c(original_features[1:10], "Diagnosis")], kernel = "radial")
predictions_svm <- predict(svm_model, newdata = TestData)
prediction_svm_cm<-table(predictions_svm,TestData$Diagnosis)
prediction_svm_er<-(prediction_svm_cm[1,2]+prediction_svm_cm[2,1])/sum(prediction_svm_cm)
accuracy_svm <- 1-prediction_svm_er
print(paste("Accuracy of Random Forest with top 10 PCA features (SMOTE best sample):",round((accuracy_svm)*100,2),"%"))

svm_model <- svm(Diagnosis ~ ., data = TrainData_SMOTE[,c(original_features[1:15], "Diagnosis")], kernel = "radial")
predictions_svm <- predict(svm_model, newdata = TestData)
prediction_svm_cm<-table(predictions_svm,TestData$Diagnosis)
prediction_svm_er<-(prediction_svm_cm[1,2]+prediction_svm_cm[2,1])/sum(prediction_svm_cm)
accuracy_svm <- 1-prediction_svm_er
print(paste("Accuracy of Random Forest with top 15 PCA features (SMOTE best sample):",round((accuracy_svm)*100,2),"%"))

svm_model <- svm(Diagnosis ~ ., data = TrainData_SMOTE[,c(original_features[1:20], "Diagnosis")], kernel = "radial")
predictions_svm <- predict(svm_model, newdata = TestData)
prediction_svm_cm<-table(predictions_svm,TestData$Diagnosis)
prediction_svm_er<-(prediction_svm_cm[1,2]+prediction_svm_cm[2,1])/sum(prediction_svm_cm)
accuracy_svm <- 1-prediction_svm_er
print(paste("Accuracy of Random Forest with top 20 PCA features (SMOTE best sample):",round((accuracy_svm)*100,2),"%"))

svm_model <- svm(Diagnosis ~ ., data = TrainData_SMOTE[,c(original_features[1:25], "Diagnosis")], kernel = "radial")
predictions_svm <- predict(svm_model, newdata = TestData)
prediction_svm_cm<-table(predictions_svm,TestData$Diagnosis)
prediction_svm_er<-(prediction_svm_cm[1,2]+prediction_svm_cm[2,1])/sum(prediction_svm_cm)
accuracy_svm <- 1-prediction_svm_er
print(paste("Accuracy of Random Forest with top 25 PCA features (SMOTE best sample):",round((accuracy_svm)*100,2),"%"))

```
```{r}
### Under-sampling
set.seed(12)
#LDA with the original Data
library(MASS)
lda_model <- lda(Diagnosis ~ ., data = TrainData_US)
predictions_lda <- predict(lda_model, newdata = TestData)
prediction_lda_cm<-table(predictions_lda$class,TestData$Diagnosis)
prediction_lda_er<-(prediction_lda_cm[1,2]+prediction_lda_cm[2,1])/sum(prediction_lda_cm)
print(paste("Accuracy of LDA:",round((1-prediction_lda_er)*100,2),"%"))

#Naive Bayes
library(e1071)
nb_model <- naiveBayes(Diagnosis ~ ., data = TrainData_US)
predictions_nb <- predict(nb_model, newdata = TestData)
prediction_nb_cm<-table(predictions_nb,TestData$Diagnosis)
prediction_nb_er<-(prediction_nb_cm[1,2]+prediction_nb_cm[2,1])/sum(prediction_nb_cm)
print(paste("Accuracy of Naive Bayes:",round((1-prediction_nb_er)*100,2),"%"))

#Decision Tree
library(rpart)
dt_model <- rpart(Diagnosis ~ ., data = TrainData_US)
predictions_dt <- predict(dt_model, newdata = TestData, type = "class")
prediction_dt_cm<-table(predictions_dt,TestData$Diagnosis)
prediction_dt_er<-(prediction_dt_cm[1,2]+prediction_dt_cm[2,1])/sum(prediction_dt_cm)
print(paste("Accuracy of Decision Tree:",round((1-prediction_dt_er)*100,2),"%"))

#Gradient Boosting Machines (GBM)
TrainData_boosting<-TrainData_US
TrainData_boosting$Diagnosis <- as.integer(TrainData_boosting$Diagnosis == "Malignant")
      #"M" will be converted to 1 and "B" to 0
library(gbm)
boosting_model <- gbm(Diagnosis ~ ., distribution = "bernoulli", data = TrainData_boosting)
predictions_boosting <- predict(boosting_model, newdata = TestData, type = "response")
predicted_classes <- ifelse(predictions_boosting > 0.5, levels(as.factor(TestData$Diagnosis))[2], levels(as.factor(TestData$Diagnosis))[1])
prediction_boosting_cm<-table(predicted_classes,TestData$Diagnosis)
prediction_boosting_er<-(prediction_boosting_cm[1,2]+prediction_boosting_cm[2,1])/sum(prediction_boosting_cm)
print(paste("Accuracy of Gradient Boosting Machines (GBM):",round((1-prediction_boosting_er)*100,2),"%"))

```
```{r}
### K-medoid clustering
set.seed(12)
#LDA with the original Data
library(MASS)
lda_model <- lda(Diagnosis ~ ., data = TrainData_KC)
predictions_lda <- predict(lda_model, newdata = TestData)
prediction_lda_cm<-table(predictions_lda$class,TestData$Diagnosis)
prediction_lda_er<-(prediction_lda_cm[1,2]+prediction_lda_cm[2,1])/sum(prediction_lda_cm)
print(paste("Accuracy of LDA:",round((1-prediction_lda_er)*100,2),"%"))

#LDA with the original Data
library(MASS)
lda_model <- lda(Diagnosis ~ ., data = TrainData_KC)
predictions_lda <- predict(lda_model, newdata = TestData)
prediction_lda_cm<-table(predictions_lda$class,TestData$Diagnosis)
prediction_lda_er<-(prediction_lda_cm[1,2]+prediction_lda_cm[2,1])/sum(prediction_lda_cm)
print(paste("Accuracy of LDA:",round((1-prediction_lda_er)*100,2),"%"))

#Naive Bayes
library(e1071)
nb_model <- naiveBayes(Diagnosis ~ ., data = TrainData_KC)
predictions_nb <- predict(nb_model, newdata = TestData)
prediction_nb_cm<-table(predictions_nb,TestData$Diagnosis)
prediction_nb_er<-(prediction_nb_cm[1,2]+prediction_nb_cm[2,1])/sum(prediction_nb_cm)
print(paste("Accuracy of Naive Bayes:",round((1-prediction_nb_er)*100,2),"%"))

#Decision Tree
library(rpart)
dt_model <- rpart(Diagnosis ~ ., data = TrainData_KC)
predictions_dt <- predict(dt_model, newdata = TestData, type = "class")
prediction_dt_cm<-table(predictions_dt,TestData$Diagnosis)
prediction_dt_er<-(prediction_dt_cm[1,2]+prediction_dt_cm[2,1])/sum(prediction_dt_cm)
print(paste("Accuracy of Decision Tree:",round((1-prediction_dt_er)*100,2),"%"))

#Gradient Boosting Machines (GBM)
TrainData_boosting<-TrainData_KC
TrainData_boosting$Diagnosis <- as.integer(TrainData_boosting$Diagnosis == "Malignant")
      #"M" will be converted to 1 and "B" to 0
library(gbm)
boosting_model <- gbm(Diagnosis ~ ., distribution = "bernoulli", data = TrainData_boosting)
predictions_boosting <- predict(boosting_model, newdata = TestData, type = "response")
predicted_classes <- ifelse(predictions_boosting > 0.5, levels(as.factor(TestData$Diagnosis))[2], levels(as.factor(TestData$Diagnosis))[1])
prediction_boosting_cm<-table(predicted_classes,TestData$Diagnosis)
prediction_boosting_er<-(prediction_boosting_cm[1,2]+prediction_boosting_cm[2,1])/sum(prediction_boosting_cm)
print(paste("Accuracy of Gradient Boosting Machines (GBM):",round((1-prediction_boosting_er)*100,2),"%"))

```
```{r}
### original data
set.seed(12)
#LDA with the original Data
library(MASS)
lda_model <- lda(Diagnosis ~ ., data = TrainData[,c(original_features[1:10], "Diagnosis")])
predictions_lda <- predict(lda_model, newdata = TestData)
prediction_lda_cm<-table(predictions_lda$class,TestData$Diagnosis)
prediction_lda_er<-(prediction_lda_cm[1,2]+prediction_lda_cm[2,1])/sum(prediction_lda_cm)
print(paste("Accuracy of LDA with to 10 features (original data):",round((1-prediction_lda_er)*100,2),"%"))

lda_model <- lda(Diagnosis ~ ., data = TrainData[,c(original_features[1:15], "Diagnosis")])
predictions_lda <- predict(lda_model, newdata = TestData)
prediction_lda_cm<-table(predictions_lda$class,TestData$Diagnosis)
prediction_lda_er<-(prediction_lda_cm[1,2]+prediction_lda_cm[2,1])/sum(prediction_lda_cm)
print(paste("Accuracy of LDA with to 15 features (original data):",round((1-prediction_lda_er)*100,2),"%"))

lda_model <- lda(Diagnosis ~ ., data = TrainData[,c(original_features[1:20], "Diagnosis")])
predictions_lda <- predict(lda_model, newdata = TestData)
prediction_lda_cm<-table(predictions_lda$class,TestData$Diagnosis)
prediction_lda_er<-(prediction_lda_cm[1,2]+prediction_lda_cm[2,1])/sum(prediction_lda_cm)
print(paste("Accuracy of LDA with to 20 features (original data):",round((1-prediction_lda_er)*100,2),"%"))

lda_model <- lda(Diagnosis ~ ., data = TrainData[,c(original_features[1:25], "Diagnosis")])
predictions_lda <- predict(lda_model, newdata = TestData)
prediction_lda_cm<-table(predictions_lda$class,TestData$Diagnosis)
prediction_lda_er<-(prediction_lda_cm[1,2]+prediction_lda_cm[2,1])/sum(prediction_lda_cm)
print(paste("Accuracy of LDA with to 25 features (original data):",round((1-prediction_lda_er)*100,2),"%"))

#Naive Bayes
library(e1071)
nb_model <- naiveBayes(Diagnosis ~ ., data = TrainData[,c(original_features[1:10], "Diagnosis")])
predictions_nb <- predict(nb_model, newdata = TestData)
prediction_nb_cm<-table(predictions_nb,TestData$Diagnosis)
prediction_nb_er<-(prediction_nb_cm[1,2]+prediction_nb_cm[2,1])/sum(prediction_nb_cm)
print(paste("Accuracy of Naive Bayes with top 10 features (original data):",round((1-prediction_nb_er)*100,2),"%"))

nb_model <- naiveBayes(Diagnosis ~ ., data = TrainData[,c(original_features[1:15], "Diagnosis")])
predictions_nb <- predict(nb_model, newdata = TestData)
prediction_nb_cm<-table(predictions_nb,TestData$Diagnosis)
prediction_nb_er<-(prediction_nb_cm[1,2]+prediction_nb_cm[2,1])/sum(prediction_nb_cm)
print(paste("Accuracy of Naive Bayes with top 15 features (original data):",round((1-prediction_nb_er)*100,2),"%"))

nb_model <- naiveBayes(Diagnosis ~ ., data = TrainData[,c(original_features[1:20], "Diagnosis")])
predictions_nb <- predict(nb_model, newdata = TestData)
prediction_nb_cm<-table(predictions_nb,TestData$Diagnosis)
prediction_nb_er<-(prediction_nb_cm[1,2]+prediction_nb_cm[2,1])/sum(prediction_nb_cm)
print(paste("Accuracy of Naive Bayes with top 20 features (original data):",round((1-prediction_nb_er)*100,2),"%"))

nb_model <- naiveBayes(Diagnosis ~ ., data = TrainData[,c(original_features[1:25], "Diagnosis")])
predictions_nb <- predict(nb_model, newdata = TestData)
prediction_nb_cm<-table(predictions_nb,TestData$Diagnosis)
prediction_nb_er<-(prediction_nb_cm[1,2]+prediction_nb_cm[2,1])/sum(prediction_nb_cm)
print(paste("Accuracy of Naive Bayes with top 25 features (original data):",round((1-prediction_nb_er)*100,2),"%"))

#Decision Tree
library(rpart)
dt_model <- rpart(Diagnosis ~ ., data = TrainData[,c(original_features[1:10], "Diagnosis")])
predictions_dt <- predict(dt_model, newdata = TestData, type = "class")
prediction_dt_cm<-table(predictions_dt,TestData$Diagnosis)
prediction_dt_er<-(prediction_dt_cm[1,2]+prediction_dt_cm[2,1])/sum(prediction_dt_cm)
print(paste("Accuracy of Decision Tree with top 10 features (original data):",round((1-prediction_dt_er)*100,2),"%"))

dt_model <- rpart(Diagnosis ~ ., data = TrainData[,c(original_features[1:15], "Diagnosis")])
predictions_dt <- predict(dt_model, newdata = TestData, type = "class")
prediction_dt_cm<-table(predictions_dt,TestData$Diagnosis)
prediction_dt_er<-(prediction_dt_cm[1,2]+prediction_dt_cm[2,1])/sum(prediction_dt_cm)
print(paste("Accuracy of Decision Tree with top 15 features (original data):",round((1-prediction_dt_er)*100,2),"%"))

dt_model <- rpart(Diagnosis ~ ., data = TrainData[,c(original_features[1:20], "Diagnosis")])
predictions_dt <- predict(dt_model, newdata = TestData, type = "class")
prediction_dt_cm<-table(predictions_dt,TestData$Diagnosis)
prediction_dt_er<-(prediction_dt_cm[1,2]+prediction_dt_cm[2,1])/sum(prediction_dt_cm)
print(paste("Accuracy of Decision Tree with top 20 features (original data):",round((1-prediction_dt_er)*100,2),"%"))

dt_model <- rpart(Diagnosis ~ ., data = TrainData[,c(original_features[1:25], "Diagnosis")])
predictions_dt <- predict(dt_model, newdata = TestData, type = "class")
prediction_dt_cm<-table(predictions_dt,TestData$Diagnosis)
prediction_dt_er<-(prediction_dt_cm[1,2]+prediction_dt_cm[2,1])/sum(prediction_dt_cm)
print(paste("Accuracy of Decision Tree with top 25 features (original data):",round((1-prediction_dt_er)*100,2),"%"))

#Gradient Boosting Machines (GBM)
TrainData_boosting<-TrainData[,c(original_features[1:10], "Diagnosis")]
TrainData_boosting$Diagnosis <- as.integer(TrainData_boosting$Diagnosis == "Malignant")
      #"M" will be converted to 1 and "B" to 0
library(gbm)
boosting_model <- gbm(Diagnosis ~ ., distribution = "bernoulli", data = TrainData_boosting)
predictions_boosting <- predict(boosting_model, newdata = TestData, type = "response")
predicted_classes <- ifelse(predictions_boosting > 0.5, levels(as.factor(TestData$Diagnosis))[2], levels(as.factor(TestData$Diagnosis))[1])
prediction_boosting_cm<-table(predicted_classes,TestData$Diagnosis)
prediction_boosting_er<-(prediction_boosting_cm[1,2]+prediction_boosting_cm[2,1])/sum(prediction_boosting_cm)
print(paste("Accuracy of Gradient Boosting Machines (GBM) with top 10 features:",round((1-prediction_boosting_er)*100,2),"%"))

TrainData_boosting<-TrainData[,c(original_features[1:15], "Diagnosis")]
TrainData_boosting$Diagnosis <- as.integer(TrainData_boosting$Diagnosis == "Malignant")
      #"M" will be converted to 1 and "B" to 0
library(gbm)
boosting_model <- gbm(Diagnosis ~ ., distribution = "bernoulli", data = TrainData_boosting)
predictions_boosting <- predict(boosting_model, newdata = TestData, type = "response")
predicted_classes <- ifelse(predictions_boosting > 0.5, levels(as.factor(TestData$Diagnosis))[2], levels(as.factor(TestData$Diagnosis))[1])
prediction_boosting_cm<-table(predicted_classes,TestData$Diagnosis)
prediction_boosting_er<-(prediction_boosting_cm[1,2]+prediction_boosting_cm[2,1])/sum(prediction_boosting_cm)
print(paste("Accuracy of Gradient Boosting Machines (GBM) with top 15 features:",round((1-prediction_boosting_er)*100,2),"%"))

TrainData_boosting<-TrainData[,c(original_features[1:20], "Diagnosis")]
TrainData_boosting$Diagnosis <- as.integer(TrainData_boosting$Diagnosis == "Malignant")
      #"M" will be converted to 1 and "B" to 0
library(gbm)
boosting_model <- gbm(Diagnosis ~ ., distribution = "bernoulli", data = TrainData_boosting)
predictions_boosting <- predict(boosting_model, newdata = TestData, type = "response")
predicted_classes <- ifelse(predictions_boosting > 0.5, levels(as.factor(TestData$Diagnosis))[2], levels(as.factor(TestData$Diagnosis))[1])
prediction_boosting_cm<-table(predicted_classes,TestData$Diagnosis)
prediction_boosting_er<-(prediction_boosting_cm[1,2]+prediction_boosting_cm[2,1])/sum(prediction_boosting_cm)
print(paste("Accuracy of Gradient Boosting Machines (GBM) with top 20 features:",round((1-prediction_boosting_er)*100,2),"%"))

TrainData_boosting<-TrainData[,c(original_features[1:25], "Diagnosis")]
TrainData_boosting$Diagnosis <- as.integer(TrainData_boosting$Diagnosis == "Malignant")
      #"M" will be converted to 1 and "B" to 0
library(gbm)
boosting_model <- gbm(Diagnosis ~ ., distribution = "bernoulli", data = TrainData_boosting)
predictions_boosting <- predict(boosting_model, newdata = TestData, type = "response")
predicted_classes <- ifelse(predictions_boosting > 0.5, levels(as.factor(TestData$Diagnosis))[2], levels(as.factor(TestData$Diagnosis))[1])
prediction_boosting_cm<-table(predicted_classes,TestData$Diagnosis)
prediction_boosting_er<-(prediction_boosting_cm[1,2]+prediction_boosting_cm[2,1])/sum(prediction_boosting_cm)
print(paste("Accuracy of Gradient Boosting Machines (GBM) with top 25 features:",round((1-prediction_boosting_er)*100,2),"%"))
```

```{r}
set.seed(12)
train.x=data.frame(TrainData[,1:30])
test.x=data.frame(TestData[,1:30])
train.y=factor(TrainData[,31])
test.y=factor(TestData[,31])

KNN.fit=knn(train.x, test.x, train.y)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with all features  (original data):",round((accuracy_KNN)*100,2),"%"))


KNN.fit=knn(train.x[,original_features[1:10]], test.x[,original_features[1:10]], train.y)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with top 10 PCA features  (original data):",round((accuracy_KNN)*100,2),"%"))


KNN.fit=knn(train.x[,original_features[1:15]], test.x[,original_features[1:15]], train.y)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with top 15 PCA features  (original data):",round((accuracy_KNN)*100,2),"%"))


KNN.fit=knn(train.x[,original_features[1:20]], test.x[,original_features[1:20]], train.y)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with top 20 PCA features  (original data):",round((accuracy_KNN)*100,2),"%"))


KNN.fit=knn(train.x[,original_features[1:25]], test.x[,original_features[1:25]], train.y)
cm_KNN<-confusionMatrix(KNN.fit,test.y)
accuracy_KNN <- cm_KNN$overall['Accuracy']
print(paste("Accuracy of KNN with top 25 PCA features  (original data):",round((accuracy_KNN)*100,2),"%"))

```
```{r}
### ROC curve

#QDA
#install.packages("pROC")
library(pROC)
TrainData_SMOTE <- data.frame(read.csv("TrainData_SMOTE_qda.csv"))
TrainData_SMOTE$Diagnosis <- as.factor(TrainData_SMOTE$Diagnosis)
qda_model <- qda(Diagnosis ~ ., data = TrainData_SMOTE)
predictions_qda <- predict(qda_model, newdata = TestData)

predicted_probabilities <- predictions_qda$posterior[, "Malignant"]
roc_curve <- roc(as.numeric(TestData$Diagnosis), predicted_probabilities)

plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "red")
auc_value <- auc(roc_curve)
print(auc_value)

#SVM
TrainData_SMOTE <- data.frame(read.csv("TrainData_SMOTE_svm.csv"))
TrainData_SMOTE$Diagnosis <- as.factor(TrainData_SMOTE$Diagnosis)
svm_model <- svm(Diagnosis ~ ., data = TrainData_SMOTE, kernel = "radial")
predictions_svm <- predict(svm_model, newdata = TestData,decision.values = TRUE)
decision_scores <- attr(predictions_svm, "decision.values")
probabilities <- 1 / (1 + exp(-decision_scores))

roc_curve <- roc(as.numeric(TestData$Diagnosis), probabilities)

plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "red")
auc_value <- auc(roc_curve)
print(auc_value)

#Random forest
set.seed(12)
TrainData_SMOTE <- data.frame(read.csv("TrainData_SMOTE_rf.csv"))
TrainData_SMOTE$Diagnosis <- as.factor(TrainData_SMOTE$Diagnosis)
model_rf <- randomForest(Diagnosis ~ ., data = TrainData_SMOTE, ntree = 100)
predictions <- predict(model_rf, newdata = TestData_OS)
robabilities <- as.numeric(model_rf$votes[, "Malignant"]) / 100

roc_curve <- roc(as.numeric(TestData_OS$Diagnosis), probabilities)

plot(roc_curve, main = "ROC Curve for Random Forest", col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "red")

auc_value <- auc(roc_curve)
print(auc_value)

##KNN

TrainData_SMOTE <- data.frame(read.csv("TrainData_SMOTE_KNN.csv"))
TrainData_SMOTE$Diagnosis <- as.factor(TrainData_SMOTE$Diagnosis)
train.x=data.frame(TrainData_SMOTE[,1:30])
TestData_OS<-TestData
test.x=data.frame(TestData_OS[,1:30])
train.y=factor(TrainData_SMOTE[,31])
test.y=factor(TestData_OS[,31])
KNN.fit=knn(train.x, test.x, train.y,k=5,prob=T)

probabilities <- attr(KNN.fit, "prob")
roc_curve <- roc(as.numeric(test.y), probabilities)

plot(roc_curve, main = "ROC Curve for KNN with SMOTE", col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "red")

auc_value <- auc(roc_curve)
print(paste("AUC for KNN with SMOTE:", auc_value))
```



